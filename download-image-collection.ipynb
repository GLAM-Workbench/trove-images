{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824b559a-0f98-49f2-b690-b09838a0a978",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Download a collection of digitised images\n",
    "\n",
    "Digitised photographs and other images are often organised into collections. While the Trove web interface does include a download option for collections, it has a number of limitations:\n",
    "\n",
    "- the images are all combined into a single zip file\n",
    "- you can generally download a maximum of 20 images at a time\n",
    "- the resolution of the downloaded images is often quite low\n",
    "\n",
    "This notebook provides an alternative method that downloads all of the available images in a collection (and any sub-collections) at the highest available resolution. The method is as follows:\n",
    "\n",
    "- the `nla.obj` identifiers for all the items in the collection are harvested from the browse interface\n",
    "- a url to download a high-resolution version of the image is constructed using each `nla.obj` id\n",
    "- each image is downloaded and saved\n",
    "\n",
    "The downloaded images will be saved in the `images/[COLLECTION ID]` folder. Once the harvest is complete, the dataset will be zipped up with an [RO-Crate](https://www.researchobject.org/ro-crate/) metadata file and a link displayed for easy download. The RO-Crate metadata file captures the context and results of the harvest.\n",
    "\n",
    "The image file names use the `nla.obj` identifiers. For example, the image of `nla.obj-147116797` is saved as `nla.obj-147116797.jpg`. The identifiers also link the image back to the website: `nla.obj-147116797.jpg` comes from `https://nla.gov.au/nla.obj-147116797`.\n",
    "\n",
    "## Finding collections of images\n",
    "\n",
    "There's no direct way of searching for *collections*, they tend to be mixed up in search results with individual images. Not all digitised images are in collections, but if they are you can use the breadcrumbs navigation to move up the hierarchy. Each level in the collection hierarchy will have it's own `nla.obj` identifier that you can use to download images from that level and below.\n",
    "\n",
    "For example, [this excellent poster](https://nla.gov.au/nla.obj-133781081) is part of a very large collection of digitised posters and exists at the bottom of the breadcrumb hierarchy: **Home > Guide to Pre-1950 Advertising Posters in the National Library of Australia digitised by the 2019 Tax Time Appeal > Poster drawers > Posters**. Clicking on 'Guide to Pre-1950 Advertising Posters', 'Poster drawers', or 'Posters' will take you to different levels in the collection hierarchy. You can then just copy the `nla.obj` identifier from the url and paste it below to download all child images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468eb33-0bad-42f7-acfd-9aec17dcaa94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mimetypes\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import ipynbname\n",
    "import nbformat\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import HTML, display\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from rocrate.rocrate import ContextEntity, ROCrate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "s = requests_cache.CachedSession(expire_after=timedelta(days=30))\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n",
    "s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a18bc-935b-4f5b-a647-5166f475968e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_url(url):\n",
    "    \"\"\"\n",
    "    Make sure nla.obj identifiers are properly formatted urls.\n",
    "    \"\"\"\n",
    "    url = re.sub(r\"https?://nla/\", \"https://nla.gov.au/\", url)\n",
    "    url = url.replace(\"\\\\\\\\\", \"//\")\n",
    "    if not url.startswith(\"http\"):\n",
    "        # print(url)\n",
    "        url = f\"https://nla.gov.au/{url.strip('/')}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_work_data(url):\n",
    "    \"\"\"\n",
    "    Extract work data in a JSON string from the work's HTML page.\n",
    "    \"\"\"\n",
    "    url = prepare_url(url)\n",
    "    try:\n",
    "        response = s.get(url)\n",
    "    except ConnectionError:\n",
    "        print(url)\n",
    "    if response.ok:\n",
    "        try:\n",
    "            work_data = re.search(\n",
    "                r\"var work = JSON\\.parse\\(JSON\\.stringify\\((\\{.*\\})\", response.text\n",
    "            ).group(1)\n",
    "        except AttributeError:\n",
    "            work_data = \"{}\"\n",
    "    else:\n",
    "        print(url)\n",
    "        work_data = \"{}\"\n",
    "    if not response.from_cache:\n",
    "        time.sleep(0.2)\n",
    "    return json.loads(work_data)\n",
    "\n",
    "\n",
    "def harvest_collection_items(collection_id, include_subcollections=False):\n",
    "    \"\"\"\n",
    "    Harvest all the items in a Trove collection (including any sub-collections)\n",
    "    by scraping the item identifiers from the 'Browse collection' pop-up.\n",
    "    See the Trove Data Guide:\n",
    "    \"\"\"\n",
    "    # The initial startIdx value\n",
    "    start = 0\n",
    "    # Number of results per page, used to increment the startIdx value\n",
    "    n = 20\n",
    "    items = []\n",
    "    # If there aren't 20 results on the page then we've reached the end, so continue harvesting until that happens.\n",
    "    while n == 20:\n",
    "        url = f\"https://nla.gov.au/{collection_id}/browse?startIdx={start}&rows=20&op=c\"\n",
    "        # Get the browse page\n",
    "        response = s.get(url)\n",
    "\n",
    "        # Beautifulsoup turns the HTML into an easily navigable structure\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find all the divs containing issue details and loop through them\n",
    "        details = soup.find_all(class_=\"l-item-info\")\n",
    "        for detail in details:\n",
    "            # Set a default type\n",
    "            item_type = \"item\"\n",
    "\n",
    "            # Look for the a tag with class \"obj-reference content\"\n",
    "            item_id = detail.find(\n",
    "                lambda tag: tag.name == \"a\"\n",
    "                and tag.get(\"class\") == [\"obj-reference\", \"content\"]\n",
    "            )[\"href\"].strip(\"/\")\n",
    "\n",
    "            # Look for a link to 'children', indicating it's a subcollection (or a book or issue with pages)\n",
    "            has_children = detail.find(\n",
    "                lambda tag: tag.name == \"a\" and tag.get(\"class\") == [\"obj-reference\"]\n",
    "            )\n",
    "\n",
    "            # If it has children, harvest items from the subcollection\n",
    "            if has_children and include_subcollections is True:\n",
    "                item_type = \"collection\"\n",
    "                items += harvest_collection_items(item_id, include_subcollections=True)\n",
    "\n",
    "            # Save the item\n",
    "            # The parent_id will enable us to identify items that are in subcollections\n",
    "            items.append(\n",
    "                {\"item_id\": item_id, \"item_type\": item_type, \"parent_id\": collection_id}\n",
    "            )\n",
    "\n",
    "        time.sleep(0.2)\n",
    "        # Increment the startIdx\n",
    "        start += n\n",
    "        # Set n to the number of results on the current page\n",
    "        n = len(details)\n",
    "    return items\n",
    "\n",
    "\n",
    "def create_rocrate(collection_id, dir_path, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Create an RO-Crate metadata file describing the downloaded dataset.\n",
    "    \"\"\"\n",
    "    crate = ROCrate()\n",
    "    crate.add_tree(dir_path)\n",
    "    nb_path = ipynbname.path()\n",
    "    nb = nbformat.read(nb_path, nbformat.NO_CONVERT)\n",
    "    metadata = nb.metadata.rocrate\n",
    "    nb_url = metadata.get(\"url\", \"\")\n",
    "    nb_properties = {\n",
    "        \"@type\": [\"File\", \"SoftwareSourceCode\"],\n",
    "        \"name\": metadata.get(\"name\", \"\"),\n",
    "        \"description\": metadata.get(\"description\", \"\"),\n",
    "        \"encodingFormat\": \"application/x-ipynb+json\",\n",
    "        \"codeRepository\": metadata.get(\"codeRepository\", \"\"),\n",
    "        \"url\": nb_url,\n",
    "    }\n",
    "    crate.add(ContextEntity(crate, nb_url, properties=nb_properties))\n",
    "    action_id = f\"{nb_path.stem}_run\"\n",
    "    action_properties = {\n",
    "        \"@type\": \"CreateAction\",\n",
    "        \"instrument\": {\"@id\": nb_url},\n",
    "        \"actionStatus\": {\"@id\": \"http://schema.org/CompletedActionStatus\"},\n",
    "        \"name\": f\"Run of notebook: {nb_path.name}\",\n",
    "        \"result\": {\"@id\": f\"{dir_path.name}/\"},\n",
    "        \"query\": collection_id,\n",
    "        \"startDate\": start_date,\n",
    "        \"endDate\": end_date,\n",
    "    }\n",
    "    crate.add(ContextEntity(crate, action_id, properties=action_properties))\n",
    "    for img in dir_path.glob(\"*.jpg\"):\n",
    "        encoding = mimetypes.guess_type(img)[0]\n",
    "        stats = img.stat()\n",
    "        size = stats.st_size\n",
    "        date = datetime.fromtimestamp(stats.st_mtime).strftime(\"%Y-%m-%d\")\n",
    "        crate.update_jsonld(\n",
    "            {\n",
    "                \"@id\": f\"images/{img.name}\",\n",
    "                \"dateModified\": date,\n",
    "                \"contentSize\": size,\n",
    "                \"encodingFormat\": encoding,\n",
    "            }\n",
    "        )\n",
    "    crate.write(dir_path.parent)\n",
    "    crate.write_zip(dir_path.parent)\n",
    "\n",
    "\n",
    "def download_image(item_id, dir_path, not_available):\n",
    "    file_path = Path(dir_path, f\"{item_id}.jpg\")\n",
    "    if not file_path.exists():\n",
    "        url = f\"https://nla.gov.au/{item_id}/image\"\n",
    "        response = s.get(url, stream=True)\n",
    "\n",
    "        # Exclude 404 responses and 'not available' images\n",
    "        if response.ok and response.content != not_available:\n",
    "            file_path.write_bytes(response.content)\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "def download_images(collection_id, create_crate=True):\n",
    "    start_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Set up a directory to save the images to\n",
    "    dir_path = Path(\"images\", collection_id, \"images\")\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Load a 'not available' image to compare with what we download\n",
    "    # If the bytes match then we won't save it\n",
    "    not_available = Path(\"not_available.jpg\").read_bytes()\n",
    "\n",
    "    # Get the image identifiers\n",
    "    items = harvest_collection_items(collection_id, include_subcollections=True)\n",
    "    for item in tqdm(items):\n",
    "        item_id = item[\"item_id\"]\n",
    "        if item[\"item_type\"] == \"item\":\n",
    "            download_image(item_id, dir_path, not_available)\n",
    "        if item[\"item_type\"] == \"collection\":\n",
    "            # Sometimes items with children also have images that aren't included amongst the children!!\n",
    "            # We need to look at the embedded metadata to check for copies\n",
    "            metadata = get_work_data(item_id)\n",
    "            if \"copies\" in metadata:\n",
    "                download_image(item_id, dir_path, not_available)\n",
    "    end_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    if create_crate is True:\n",
    "        create_rocrate(collection_id, dir_path, start_date, end_date)\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"Download dataset: <a href='images/{collection_id}.zip', download>images/{collection_id}.zip</a>\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d316e2-c331-4860-8698-c8c3eccaeb4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "nbval-skip"
    ]
   },
   "outputs": [],
   "source": [
    "download_images(\"nla.obj-2590820305\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a64c8-afcf-4f49-96bb-1693e7c038b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS CELL -- TESTING ONLY\n",
    "\n",
    "if os.getenv(\"GW_STATUS\") == \"dev\":\n",
    "    # ipynbname won't work in testing env, so don't create the crate\n",
    "    download_images(\"nla.obj-2590820305\", create_crate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44d9fb-dc30-4609-ad25-cdc33195cd34",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.au/) for the [GLAM Workbench](https://glam-workbench.net/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rocrate": {
   "author": [
    {
     "mainEntityOfPage": "https://timsherratt.au",
     "name": "Sherratt, Tim",
     "orcid": "https://orcid.org/0000-0001-7956-4498"
    }
   ],
   "description": "Digitised photographs and other images are often organised into collections. While the Trove web interface does include a download option for collections, it has a number of limitations. This notebook provides an alternative method that downloads all of the available images in a collection (and any sub-collections) at the highest available resolution.",
   "mainEntityOfPage": "https://glam-workbench.net/trove-images/download-image-collection/",
   "name": "Download a collection of digitised images",
   "url": "https://github.com/GLAM-Workbench/trove-images/blob/master/download-image-collection.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
